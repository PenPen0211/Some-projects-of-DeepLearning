{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于LSTM的IMDB影评集情感分类任务\n",
    "### 数据科学与大数据技术二班&emsp;廖寒曦\n",
    "#### 在本机cpu:M1 &emsp;python v3.8.6&emsp; tensorflow v2.4.0上执行约两小时达到训练要求后退出训练，测试集上Accuracy为0.805，通过对超参数设置可节约训练时间\n",
    "#### 运行结果：\n",
    "##### INFO:tensorflow:Best Accuracy: 0.805<br/>INFO:tensorflow:Testing Accuracy not improved over 3 epochs, Early Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、配置环境与数据预处理\n",
    "#### （一）导入需要的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "'''\n",
    "Filename: Assignment_RNN.ipynb\n",
    "Author  : 廖寒曦\n",
    "Time    : 2022/04/02 09:59:30\n",
    "'''\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "import logging\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （二）数据预处理\n",
    "从keras包中导入IMDB数据,其结构为嵌套列表，子列表为一条影评，而子列表中元素为词在对应词库中的索引,需要将词索引转换为词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小为25000\n",
      "测试集大小为25000\n"
     ]
    }
   ],
   "source": [
    "#从keras包里倒入所需的imdb影评集\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "\n",
    "print(\"训练集大小为{}\".format(len(x_train)))\n",
    "print(\"测试集大小为{}\".format(len(x_test)))\n",
    "\n",
    "#载入imdb数据集原始数据索引表\n",
    "_word2idx = tf.keras.datasets.imdb.get_word_index()\n",
    "#将所有数据往后移三个位置\n",
    "word2idx = {w:i+3 for w , i in _word2idx.items()}\n",
    "#补位标识符\n",
    "word2idx['<pad>'] = 0\n",
    "#每个样本开始索引\n",
    "word2idx['<start>'] = 1\n",
    "#预训练模型，有些没对应上的词的特殊字符\n",
    "word2idx['<unk>'] = 2\n",
    "idx2word = {i:w for w , i in word2idx.items() } \n",
    "#健为单词，值为索引号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先将每条影评从短到长进行排序，再将词索引转换为可读文本，构建一个计数器，记录影评中出现单词的词频，再将每条影评中词频小于10的单词删除。为避免操作过程中出现变量丢失，将训练与测试集保存至本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of words: <class 'list'>\n",
      "the size of words (Vocab Size): 20598\n"
     ]
    }
   ],
   "source": [
    "#按文本长度大小进行排序\n",
    "def sort_by_len(x,y):\n",
    "    x,y = np.asarray(x),np.asarray(y)\n",
    "    idx = sorted(range(len(x)),key = lambda i :len(x[i]))\n",
    "    return x[idx],y[idx]\n",
    "#短的在前，长的在后\n",
    "x_train,y_train = sort_by_len(x_train,y_train)\n",
    "x_test,y_test = sort_by_len(x_test,y_test)\n",
    "\n",
    "#将文本数据保存至本地文件\n",
    "def write_file(f_path,xs,ys):\n",
    "    with open(f_path,'w',encoding='utf8') as f:\n",
    "        for x,y in zip(xs,ys):\n",
    "            f.write(str(y)+ '\\t' + ' '.join([idx2word[i] for i in x][1:])+'\\n')#制符表\\t，label在前，其后是影评，影评是由原始train数据集中的索引对应产生\n",
    "     \n",
    "write_file('/Users/liaohanxi/Desktop/sentiment_classfication/train.txt',x_train,y_train)\n",
    "write_file('/Users/liaohanxi/Desktop/sentiment_classfication/test.txt',x_test,y_test)\n",
    "counter = Counter()#创建计数器对象，返回值是一个字典，字典的键是传入参数的元素，字典的值是该元素的出现次数\n",
    "\n",
    "with open (\"/Users/liaohanxi/Desktop/sentiment_classfication/train.txt\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()#对字符串调用该方法，删除右起的指定字符，中间的字符不会删除，相对的还有 左起删除lstrip，前后缀删除strip\n",
    "        label , words = line.split('\\t')#以制表符对每行影评进行分割，制表符之前的0/1是label，制表符之后的值为words\n",
    "        words = words.split(' ')#将每行影评拆分成独立的words\n",
    "        counter.update(words)#对计数器对象进行更新\n",
    "\n",
    "words = ['<pad>'] + [w for w,freq in counter.most_common() if freq >= 10]\n",
    "#counter对象的most_common方法返回一个元祖，(a,b)，a就是前述的元素，b为该元素的出现次数\n",
    "#随后选出所有词频大于10的词\n",
    "#words就是我们的词库，查看words的类型和大小\n",
    "print('the type of words:',type(words))\n",
    "print('the size of words (Vocab Size):',len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从网上下载预训练模型，此处采用的是斯坦福大学的词向量模型（glove.6B.50d.txt,该文件在网上可下载），每个单词的维度为50维，模型中可匹配单词为40万，将处理过的影评词汇与词向量模型中单词比对，其中19697个单词是已有向量表示的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-At line 0\n",
      "-At line 100000\n",
      "-At line 200000\n",
      "-At line 300000\n",
      "[19676 / 20598]words have found in pre-trained values\n"
     ]
    }
   ],
   "source": [
    "Path('/Users/liaohanxi/Desktop/sentiment_classfication/vocab').mkdir(exist_ok=True)#创建目录，并将词库保存至本地\n",
    "\n",
    "with open('/Users/liaohanxi/Desktop/sentiment_classfication/vocab/word.txt','w',encoding='utf8') as f:\n",
    "    for w in words:\n",
    "        f.write(w+'\\n')\n",
    "        word2idx = {}\n",
    "with open('/Users/liaohanxi/Desktop/sentiment_classfication/vocab/word.txt',encoding='utf-8') as f:\n",
    "    for i , line in enumerate(f):#enumerate 对可迭代对象，返回索引和值\n",
    "        line = line.rstrip()\n",
    "        word2idx[line] = i\n",
    "\n",
    "embedding = np.zeros((len(word2idx)+1,50))#对于影评集中不在预料表中的词，就是unk\n",
    "\n",
    "with open('/Users/liaohanxi/Desktop/sentiment_classfication/glove.6B/glove.6B.50d.txt',encoding='utf-8') as f:#读取预训练的词向量数据集\n",
    "    count = 0\n",
    "    for i , line in enumerate(f):#i是word\n",
    "        if i % 100000 ==0:\n",
    "            print('-At line {}'.format(i))\n",
    "        line = line.rstrip()\n",
    "        sp = line.split(' ')\n",
    "        word, vec = sp[0],sp[1:]\n",
    "        if word in word2idx:\n",
    "            count +=1\n",
    "            embedding[word2idx[word]] = np.asarray(vec, dtype='float32')#将词转换为对应的向量\n",
    "\n",
    "#打印已找到的词的信息\n",
    "print('[%d / %d]words have found in pre-trained values'%(count, len(word2idx)))\n",
    "np.save('/Users/liaohanxi/Desktop/sentiment_classfication/vocab/word.npy',embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、搭建模型\n",
    "#### （一）制作数据生成器\n",
    "制作数据生成器，模型训练过程中，每调用一次便会产生一个含评价标签的影评样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(f_path,params):\n",
    "    with open(f_path,encoding='utf-8') as f:\n",
    "        print('Reading',f_path)\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            label, text = line.split('\\t')#每一行拆分成label和setence\n",
    "            text = text.split(' ')#将text按word拆分\n",
    "            x = [params['word2idx'].get(w, len(word2idx)) for w in text]#得到当前词对应的ID，并将其填入列表x，x就表示一个句子\n",
    "            #若影评长度小于预设最大长度，则进行结断操作\n",
    "            if len(x) >= params['max_len']:\n",
    "                x = x[:params['max_len']]\n",
    "            else:\n",
    "                x += [0] * (params['max_len'] - len(x))#若小于给顶长度，则补零，零对应字符pad\n",
    "            y = int(label)\n",
    "            yield x,y#yield会顺序产生数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （二）生成数据集\n",
    "此处接口经过设计可以识别训练集和测试集，若参数is_training值为True，则生成器从训练集中获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(is_training,params):#第一个参数表示是否是用于训练，改为false则将模型用于测试数据生成\n",
    "    _shapes = ([params['max_len']],())\n",
    "    _types = (tf.int32, tf.int32)#将word以索引格式输出\n",
    "\n",
    "    if is_training:\n",
    "        ds = tf.data.Dataset.from_generator(#必须先定义好生成器才能在from_generator调用\n",
    "            lambda: data_generator(params['train_path'],params),\n",
    "            output_shapes = _shapes,\n",
    "            output_types = _types,)\n",
    "        ds = ds.shuffle(params['num_samples'])\n",
    "        ds = ds.batch(params['batch_size'])\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: data_generator(params['test_path'],params),\n",
    "            output_shapes = _shapes,\n",
    "            output_types = _types,)\n",
    "        ds = ds.shuffle(params['num_samples'])#shuffle洗牌\n",
    "        ds = ds.batch(params['batch_size'])\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)#设置缓存序列，动态根据cpu状态设置并行调用的数量\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （三）搭建网络模型\n",
    "此处采用函数型搭建，以便与后续外接使用。网络为三层双向LSTM结构，最后衔接全联接层。为避免过拟合，在每层都会删除部分cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self,params):#初始化\n",
    "        super().__init__()\n",
    "        #进行词嵌入\n",
    "        self.embedding = tf.Variable(np.load('/Users/liaohanxi/Desktop/sentiment_classfication/vocab/word.npy'),dtype=tf.float32,name='pretrained_emdedding',trainable=False,)\n",
    "        #防止神经网络过拟合，删除部分神经元，减小对权重w的依赖，删除比例就是这里的dropout_rate\n",
    "        self.drop1 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop2 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop3 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        #rnn层，采用LSTM\n",
    "        self.rnn1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'],return_sequences=True))#设定units，即每个隐层神经元数量，设定返回hidden_layer值序列给下一层\n",
    "        self.rnn2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'],return_sequences=True))#在外层用bidirectional封装\n",
    "        self.rnn3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'],return_sequences=False))#设为false 只需要输出最后一个值\n",
    "        #\n",
    "        self.drop_fc = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.fc = tf.keras.layers.Dense(2*params['rnn_units'],tf.nn.elu)\n",
    "\n",
    "        self.out_linear = tf.keras.layers.Dense(2)#两个结果即正负例的概率\n",
    "\n",
    "    def call(self,inputs,training=False):\n",
    "        if inputs.dtype != tf.int32:\n",
    "            inputs = tf.cast(inputs,tf.int32)\n",
    "\n",
    "        batch_sz = tf.shape(inputs)[0]\n",
    "        rnn_units = 2*params['rnn_units']\n",
    "\n",
    "        x = tf.nn.embedding_lookup(self.embedding,inputs)#[batch_size,max_len,size_of_vec]\n",
    "\n",
    "        x = self.drop1(x, training = training)\n",
    "        x = self.rnn1(x)\n",
    "\n",
    "        x = self.drop2(x, training = training)\n",
    "        x = self.rnn2(x)\n",
    "\n",
    "        x = self.drop3(x, training = training)\n",
    "        x = self.rnn3(x)\n",
    "\n",
    "        x = self.drop_fc(x,training = training)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = self.out_linear(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、训练模型\n",
    "#### （一）设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'vocab_path':'/Users/liaohanxi/Desktop/sentiment_classfication/vocab/word.txt',\n",
    "    'train_path':'/Users/liaohanxi/Desktop/sentiment_classfication/train.txt',\n",
    "    'test_path':'/Users/liaohanxi/Desktop/sentiment_classfication/test.txt',\n",
    "    'num_samples':25000,#数据集大小\n",
    "    'num_labels':2,\n",
    "    'batch_size':256,\n",
    "    'max_len':100,#样本最大长度\n",
    "    'rnn_units':64,#模型每层cell个数\n",
    "    'dropout_rate':0.2,\n",
    "    'clip_norm':10.,#梯度截断，防止过拟合\n",
    "    'num_patience':3,#3次未下降就停止\n",
    "    'lr':3e-4#学习率\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （二）预设置\n",
    "预设一个退出训练条件，若超过num_patience步训练，精度仍未提升，则停止训练。配置一个执行日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置一个停止训练的判断条件，若连续\n",
    "def is_descending(history: list):\n",
    "    history = history[-(params['num_patience']+1):]\n",
    "    for i in range(1,len(history)):\n",
    "        if history[i-1] <= history[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#导入   \n",
    "word2idx = {}\n",
    "with open(params['vocab_path'],encoding='utf-8') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        line = line.rstrip()\n",
    "        word2idx[line] = i\n",
    "params['word2idx'] = word2idx\n",
    "params['vocab_size'] = len(word2idx) + 1\n",
    "\n",
    "#将搭建好的模型实例化\n",
    "model = Model(params)\n",
    "model.build(input_shape=(None,None))#设置输入数据的size，fit也能自动识别\n",
    "\n",
    "#设置学习率衰减\n",
    "decay_lr = tf.optimizers.schedules.ExponentialDecay(params['lr'],1000,0.95)\n",
    "optim = tf.optimizers.Adam(params['lr'])\n",
    "global_step = 0\n",
    "\n",
    "#设置一个精确的历史列表，便于寻找最好的训练模型\n",
    "history_acc = []\n",
    "best_acc = .0 \n",
    "\n",
    "t0 = time.time()\n",
    "logger = logging.getLogger('tensorflow')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （三）执行训练\n",
    "训练时每完成一次训练就会在测试集上验证，达到预设的退出条件后，才会退出训练，其中每50步会打印一次执行日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional multiple                  58880     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection multiple                  98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection multiple                  98816     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  258       \n",
      "=================================================================\n",
      "Total params: 1,303,232\n",
      "Trainable params: 273,282\n",
      "Non-trainable params: 1,029,950\n",
      "_________________________________________________________________\n",
      "None\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 00:45:46.834535: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-06 00:45:46.834654: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:step 0 | Loss: 0.7137 | Spent: 3.8 secs | LR: 0.000300\n",
      "INFO:tensorflow:step 50 | Loss: 0.6410 | Spent: 62.6 secs | LR: 0.000299\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.697\n",
      "INFO:tensorflow:Best Accuracy: 0.697\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 100 | Loss: 0.5584 | Spent: 116.0 secs | LR: 0.000298\n",
      "INFO:tensorflow:step 150 | Loss: 0.5446 | Spent: 61.8 secs | LR: 0.000298\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.723\n",
      "INFO:tensorflow:Best Accuracy: 0.723\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 200 | Loss: 0.5873 | Spent: 114.9 secs | LR: 0.000297\n",
      "INFO:tensorflow:step 250 | Loss: 0.5088 | Spent: 63.1 secs | LR: 0.000296\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.733\n",
      "INFO:tensorflow:Best Accuracy: 0.733\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 300 | Loss: 0.5488 | Spent: 115.5 secs | LR: 0.000295\n",
      "INFO:tensorflow:step 350 | Loss: 0.5141 | Spent: 63.4 secs | LR: 0.000295\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.738\n",
      "INFO:tensorflow:Best Accuracy: 0.738\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 400 | Loss: 0.5558 | Spent: 115.3 secs | LR: 0.000294\n",
      "INFO:tensorflow:step 450 | Loss: 0.5786 | Spent: 63.4 secs | LR: 0.000293\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.748\n",
      "INFO:tensorflow:Best Accuracy: 0.748\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 500 | Loss: 0.4858 | Spent: 114.9 secs | LR: 0.000292\n",
      "INFO:tensorflow:step 550 | Loss: 0.6062 | Spent: 61.8 secs | LR: 0.000292\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.757\n",
      "INFO:tensorflow:Best Accuracy: 0.757\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 600 | Loss: 0.4345 | Spent: 115.4 secs | LR: 0.000291\n",
      "INFO:tensorflow:step 650 | Loss: 0.5434 | Spent: 63.1 secs | LR: 0.000290\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.766\n",
      "INFO:tensorflow:Best Accuracy: 0.766\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 700 | Loss: 0.4896 | Spent: 119.0 secs | LR: 0.000289\n",
      "INFO:tensorflow:step 750 | Loss: 0.4604 | Spent: 64.5 secs | LR: 0.000289\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.766\n",
      "INFO:tensorflow:Best Accuracy: 0.766\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 800 | Loss: 0.4786 | Spent: 117.6 secs | LR: 0.000288\n",
      "INFO:tensorflow:step 850 | Loss: 0.4533 | Spent: 64.4 secs | LR: 0.000287\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.771\n",
      "INFO:tensorflow:Best Accuracy: 0.771\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 900 | Loss: 0.5109 | Spent: 117.0 secs | LR: 0.000286\n",
      "INFO:tensorflow:step 950 | Loss: 0.4940 | Spent: 64.5 secs | LR: 0.000286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 01:14:00.190566: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-04-06 01:14:00.192765: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-04-06 01:14:00.194823: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-04-06 01:14:00.196875: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-04-06 01:14:00.197121: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-04-06 01:14:05.046288: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 964).\n",
      "2022-04-06 01:14:05.047806: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 964).\n",
      "2022-04-06 01:14:05.049658: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 964).\n",
      "2022-04-06 01:14:05.051766: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 964).\n",
      "2022-04-06 01:14:05.052030: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 964).\n",
      "2022-04-06 01:14:07.615199: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-04-06 01:14:07.616871: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-04-06 01:14:07.620715: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-04-06 01:14:07.622704: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-04-06 01:14:07.622942: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-04-06 01:14:10.058665: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 968).\n",
      "2022-04-06 01:14:10.060339: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 968).\n",
      "2022-04-06 01:14:10.062266: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 968).\n",
      "2022-04-06 01:14:10.064489: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 968).\n",
      "2022-04-06 01:14:10.064742: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 968).\n",
      "2022-04-06 01:14:11.273596: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 969).\n",
      "2022-04-06 01:14:11.275232: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 969).\n",
      "2022-04-06 01:14:11.277283: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 969).\n",
      "2022-04-06 01:14:11.279306: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 969).\n",
      "2022-04-06 01:14:11.279543: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 969).\n",
      "2022-04-06 01:14:13.723394: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 971).\n",
      "2022-04-06 01:14:13.725636: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 971).\n",
      "2022-04-06 01:14:13.728548: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 971).\n",
      "2022-04-06 01:14:13.730527: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 971).\n",
      "2022-04-06 01:14:13.730777: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 971).\n",
      "2022-04-06 01:14:14.935316: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 972).\n",
      "2022-04-06 01:14:14.936985: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 972).\n",
      "2022-04-06 01:14:14.939125: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 972).\n",
      "2022-04-06 01:14:14.941096: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 972).\n",
      "2022-04-06 01:14:14.941338: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 972).\n",
      "2022-04-06 01:14:16.151944: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 973).\n",
      "2022-04-06 01:14:16.153492: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 973).\n",
      "2022-04-06 01:14:16.155346: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 973).\n",
      "2022-04-06 01:14:16.157825: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 973).\n",
      "2022-04-06 01:14:16.158068: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 973).\n",
      "2022-04-06 01:14:18.746089: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 975).\n",
      "2022-04-06 01:14:18.747753: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 975).\n",
      "2022-04-06 01:14:18.749750: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 975).\n",
      "2022-04-06 01:14:18.751801: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 975).\n",
      "2022-04-06 01:14:18.752048: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 975).\n",
      "2022-04-06 01:14:19.957810: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 976).\n",
      "2022-04-06 01:14:19.959499: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 976).\n",
      "2022-04-06 01:14:19.961865: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 976).\n",
      "2022-04-06 01:14:19.963821: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 976).\n",
      "2022-04-06 01:14:19.964058: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 976).\n",
      "2022-04-06 01:14:21.179633: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 977).\n",
      "2022-04-06 01:14:21.181195: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 977).\n",
      "2022-04-06 01:14:21.183091: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 977).\n",
      "2022-04-06 01:14:21.185113: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 977).\n",
      "2022-04-06 01:14:21.185370: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 977).\n",
      "2022-04-06 01:14:22.393514: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 978).\n",
      "2022-04-06 01:14:22.395035: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 978).\n",
      "2022-04-06 01:14:22.396967: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 978).\n",
      "2022-04-06 01:14:22.398908: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 978).\n",
      "2022-04-06 01:14:22.399163: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 978).\n",
      "2022-04-06 01:14:23.604692: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 979).\n",
      "2022-04-06 01:14:23.606216: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 979).\n",
      "2022-04-06 01:14:23.608208: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 979).\n",
      "2022-04-06 01:14:23.610586: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 979).\n",
      "2022-04-06 01:14:23.610818: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 979).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.779\n",
      "INFO:tensorflow:Best Accuracy: 0.779\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 01:15:18.231365: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-04-06 01:15:18.233171: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-04-06 01:15:18.234998: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-04-06 01:15:18.237438: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-04-06 01:15:18.237681: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-04-06 01:15:19.472850: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 982).\n",
      "2022-04-06 01:15:19.474643: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 982).\n",
      "2022-04-06 01:15:19.476691: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 982).\n",
      "2022-04-06 01:15:19.478684: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 982).\n",
      "2022-04-06 01:15:19.478907: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 982).\n",
      "2022-04-06 01:15:20.690565: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 983).\n",
      "2022-04-06 01:15:20.692741: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 983).\n",
      "2022-04-06 01:15:20.695067: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 983).\n",
      "2022-04-06 01:15:20.697151: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 983).\n",
      "2022-04-06 01:15:20.697395: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 983).\n",
      "2022-04-06 01:15:22.233289: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 984).\n",
      "2022-04-06 01:15:22.235660: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 984).\n",
      "2022-04-06 01:15:22.238313: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 984).\n",
      "2022-04-06 01:15:22.240480: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 984).\n",
      "2022-04-06 01:15:22.240729: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 984).\n",
      "2022-04-06 01:15:23.489608: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 985).\n",
      "2022-04-06 01:15:23.491437: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 985).\n",
      "2022-04-06 01:15:23.494353: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 985).\n",
      "2022-04-06 01:15:23.496467: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 985).\n",
      "2022-04-06 01:15:23.496715: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 985).\n",
      "2022-04-06 01:15:24.704374: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 986).\n",
      "2022-04-06 01:15:24.706015: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 986).\n",
      "2022-04-06 01:15:24.707978: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 986).\n",
      "2022-04-06 01:15:24.709875: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 986).\n",
      "2022-04-06 01:15:24.710109: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:690] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 986).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:step 1000 | Loss: 0.4360 | Spent: 114.2 secs | LR: 0.000285\n",
      "INFO:tensorflow:step 1050 | Loss: 0.4647 | Spent: 61.6 secs | LR: 0.000284\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.779\n",
      "INFO:tensorflow:Best Accuracy: 0.779\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 1100 | Loss: 0.3899 | Spent: 114.6 secs | LR: 0.000284\n",
      "INFO:tensorflow:step 1150 | Loss: 0.4698 | Spent: 61.7 secs | LR: 0.000283\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.782\n",
      "INFO:tensorflow:Best Accuracy: 0.782\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 1200 | Loss: 0.4715 | Spent: 114.2 secs | LR: 0.000282\n",
      "INFO:tensorflow:step 1250 | Loss: 0.4833 | Spent: 61.8 secs | LR: 0.000281\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.790\n",
      "INFO:tensorflow:Best Accuracy: 0.790\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 1300 | Loss: 0.4262 | Spent: 114.8 secs | LR: 0.000281\n",
      "INFO:tensorflow:step 1350 | Loss: 0.4562 | Spent: 62.0 secs | LR: 0.000280\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.787\n",
      "INFO:tensorflow:Best Accuracy: 0.790\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 1400 | Loss: 0.4687 | Spent: 116.7 secs | LR: 0.000279\n",
      "INFO:tensorflow:step 1450 | Loss: 0.4470 | Spent: 63.2 secs | LR: 0.000278\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.773\n",
      "INFO:tensorflow:Best Accuracy: 0.790\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 1500 | Loss: 0.4538 | Spent: 114.8 secs | LR: 0.000278\n",
      "INFO:tensorflow:step 1550 | Loss: 0.4456 | Spent: 62.1 secs | LR: 0.000277\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.781\n",
      "INFO:tensorflow:Best Accuracy: 0.790\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 1600 | Loss: 0.3724 | Spent: 114.3 secs | LR: 0.000276\n",
      "INFO:tensorflow:step 1650 | Loss: 0.3943 | Spent: 62.0 secs | LR: 0.000276\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.752\n",
      "INFO:tensorflow:Best Accuracy: 0.790\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 1700 | Loss: 0.4092 | Spent: 114.8 secs | LR: 0.000275\n",
      "INFO:tensorflow:step 1750 | Loss: 0.3604 | Spent: 62.7 secs | LR: 0.000274\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.772\n",
      "INFO:tensorflow:Best Accuracy: 0.790\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 1800 | Loss: 0.4454 | Spent: 116.3 secs | LR: 0.000274\n",
      "INFO:tensorflow:step 1850 | Loss: 0.4986 | Spent: 63.0 secs | LR: 0.000273\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.795\n",
      "INFO:tensorflow:Best Accuracy: 0.795\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 1900 | Loss: 0.4701 | Spent: 115.2 secs | LR: 0.000272\n",
      "INFO:tensorflow:step 1950 | Loss: 0.4749 | Spent: 62.7 secs | LR: 0.000271\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.793\n",
      "INFO:tensorflow:Best Accuracy: 0.795\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2000 | Loss: 0.4731 | Spent: 115.2 secs | LR: 0.000271\n",
      "INFO:tensorflow:step 2050 | Loss: 0.4757 | Spent: 63.5 secs | LR: 0.000270\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.800\n",
      "INFO:tensorflow:Best Accuracy: 0.800\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2100 | Loss: 0.4905 | Spent: 114.3 secs | LR: 0.000269\n",
      "INFO:tensorflow:step 2150 | Loss: 0.4443 | Spent: 62.5 secs | LR: 0.000269\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.790\n",
      "INFO:tensorflow:Best Accuracy: 0.800\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2200 | Loss: 0.4530 | Spent: 115.7 secs | LR: 0.000268\n",
      "INFO:tensorflow:step 2250 | Loss: 0.5315 | Spent: 61.9 secs | LR: 0.000267\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.774\n",
      "INFO:tensorflow:Best Accuracy: 0.800\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2300 | Loss: 0.4410 | Spent: 114.6 secs | LR: 0.000267\n",
      "INFO:tensorflow:step 2350 | Loss: 0.4443 | Spent: 61.4 secs | LR: 0.000266\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.800\n",
      "INFO:tensorflow:Best Accuracy: 0.800\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2400 | Loss: 0.4317 | Spent: 113.9 secs | LR: 0.000265\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.797\n",
      "INFO:tensorflow:Best Accuracy: 0.800\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2450 | Loss: 0.4140 | Spent: 114.3 secs | LR: 0.000265\n",
      "INFO:tensorflow:step 2500 | Loss: 0.3730 | Spent: 62.0 secs | LR: 0.000264\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.800\n",
      "INFO:tensorflow:Best Accuracy: 0.800\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2550 | Loss: 0.5074 | Spent: 114.7 secs | LR: 0.000263\n",
      "INFO:tensorflow:step 2600 | Loss: 0.3842 | Spent: 61.1 secs | LR: 0.000263\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.798\n",
      "INFO:tensorflow:Best Accuracy: 0.800\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2650 | Loss: 0.3415 | Spent: 113.5 secs | LR: 0.000262\n",
      "INFO:tensorflow:step 2700 | Loss: 0.4094 | Spent: 61.5 secs | LR: 0.000261\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.799\n",
      "INFO:tensorflow:Best Accuracy: 0.800\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2750 | Loss: 0.4569 | Spent: 113.9 secs | LR: 0.000261\n",
      "INFO:tensorflow:step 2800 | Loss: 0.4224 | Spent: 61.7 secs | LR: 0.000260\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.803\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2850 | Loss: 0.4002 | Spent: 114.7 secs | LR: 0.000259\n",
      "INFO:tensorflow:step 2900 | Loss: 0.4105 | Spent: 62.1 secs | LR: 0.000259\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.801\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 2950 | Loss: 0.4175 | Spent: 115.0 secs | LR: 0.000258\n",
      "INFO:tensorflow:step 3000 | Loss: 0.3095 | Spent: 63.2 secs | LR: 0.000257\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.775\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3050 | Loss: 0.3764 | Spent: 114.8 secs | LR: 0.000257\n",
      "INFO:tensorflow:step 3100 | Loss: 0.4268 | Spent: 61.4 secs | LR: 0.000256\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.796\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3150 | Loss: 0.4452 | Spent: 114.2 secs | LR: 0.000255\n",
      "INFO:tensorflow:step 3200 | Loss: 0.3867 | Spent: 61.3 secs | LR: 0.000255\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.803\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3250 | Loss: 0.4037 | Spent: 113.8 secs | LR: 0.000254\n",
      "INFO:tensorflow:step 3300 | Loss: 0.3824 | Spent: 61.5 secs | LR: 0.000253\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.780\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3350 | Loss: 0.3568 | Spent: 115.4 secs | LR: 0.000253\n",
      "INFO:tensorflow:step 3400 | Loss: 0.3841 | Spent: 61.8 secs | LR: 0.000252\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.774\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3450 | Loss: 0.4137 | Spent: 115.0 secs | LR: 0.000251\n",
      "INFO:tensorflow:step 3500 | Loss: 0.3676 | Spent: 61.6 secs | LR: 0.000251\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.802\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3550 | Loss: 0.3192 | Spent: 114.8 secs | LR: 0.000250\n",
      "INFO:tensorflow:step 3600 | Loss: 0.4057 | Spent: 62.0 secs | LR: 0.000249\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.801\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3650 | Loss: 0.4190 | Spent: 114.6 secs | LR: 0.000249\n",
      "INFO:tensorflow:step 3700 | Loss: 0.3826 | Spent: 61.2 secs | LR: 0.000248\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.790\n",
      "INFO:tensorflow:Best Accuracy: 0.803\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3750 | Loss: 0.3535 | Spent: 113.9 secs | LR: 0.000248\n",
      "INFO:tensorflow:step 3800 | Loss: 0.3532 | Spent: 62.1 secs | LR: 0.000247\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.805\n",
      "INFO:tensorflow:Best Accuracy: 0.805\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3850 | Loss: 0.3580 | Spent: 115.2 secs | LR: 0.000246\n",
      "INFO:tensorflow:step 3900 | Loss: 0.3398 | Spent: 63.6 secs | LR: 0.000246\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.765\n",
      "INFO:tensorflow:Best Accuracy: 0.805\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 3950 | Loss: 0.4013 | Spent: 114.3 secs | LR: 0.000245\n",
      "INFO:tensorflow:step 4000 | Loss: 0.4391 | Spent: 62.2 secs | LR: 0.000244\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.801\n",
      "INFO:tensorflow:Best Accuracy: 0.805\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 4050 | Loss: 0.4231 | Spent: 114.5 secs | LR: 0.000244\n",
      "INFO:tensorflow:step 4100 | Loss: 0.3442 | Spent: 61.9 secs | LR: 0.000243\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.775\n",
      "INFO:tensorflow:Best Accuracy: 0.805\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 4150 | Loss: 0.3758 | Spent: 115.0 secs | LR: 0.000242\n",
      "INFO:tensorflow:step 4200 | Loss: 0.3751 | Spent: 62.6 secs | LR: 0.000242\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.797\n",
      "INFO:tensorflow:Best Accuracy: 0.805\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 4250 | Loss: 0.3819 | Spent: 114.3 secs | LR: 0.000241\n",
      "INFO:tensorflow:step 4300 | Loss: 0.3992 | Spent: 61.4 secs | LR: 0.000241\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.802\n",
      "INFO:tensorflow:Best Accuracy: 0.805\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 4350 | Loss: 0.3944 | Spent: 114.6 secs | LR: 0.000240\n",
      "INFO:tensorflow:step 4400 | Loss: 0.3404 | Spent: 61.7 secs | LR: 0.000239\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.789\n",
      "INFO:tensorflow:Best Accuracy: 0.805\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 4450 | Loss: 0.3389 | Spent: 113.4 secs | LR: 0.000239\n",
      "INFO:tensorflow:step 4500 | Loss: 0.3775 | Spent: 61.2 secs | LR: 0.000238\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.788\n",
      "INFO:tensorflow:Best Accuracy: 0.805\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/train.txt\n",
      "INFO:tensorflow:step 4550 | Loss: 0.2973 | Spent: 113.2 secs | LR: 0.000238\n",
      "INFO:tensorflow:step 4600 | Loss: 0.3980 | Spent: 61.0 secs | LR: 0.000237\n",
      "Reading /Users/liaohanxi/Desktop/sentiment_classfication/test.txt\n",
      "INFO:tensorflow:Evalution: Testing Accuracy: 0.787\n",
      "INFO:tensorflow:Best Accuracy: 0.805\n",
      "INFO:tensorflow:Testing Accuracy not improved over 3 epochs, Early Stop\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "while True:\n",
    "    for texts, labels in dataset(is_training=True, params=params):\n",
    "        with tf.GradientTape() as tape:#记录运算中的梯度\n",
    "            logits = model(texts, training=True)\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        optim.lr.assign(decay_lr(global_step))\n",
    "        grads = tape.gradient(loss,model.trainable_variables)\n",
    "        grads,_ = tf.clip_by_global_norm(grads,params['clip_norm'])\n",
    "        optim.apply_gradients(zip(grads,model.trainable_variables))#将算出的梯度与对应的待更新参数配对\n",
    "\n",
    "        if global_step % 50 == 0:\n",
    "            logger.info(\"step {} | Loss: {:.4f} | Spent: {:.1f} secs | LR: {:.6f}\".format(global_step,loss.numpy().item(),time.time()-t0,optim.lr.numpy().item()))\n",
    "            t0 = time.time()\n",
    "        global_step +=1\n",
    "\n",
    "    m = tf.keras.metrics.Accuracy()\n",
    "\n",
    "    for texts,labels in dataset(is_training=False,params=params):\n",
    "        logits = model(texts,training=False)\n",
    "        y_pred = tf.argmax(logits,axis=-1)\n",
    "        m.update_state(y_true=labels,y_pred=y_pred)\n",
    "\n",
    "    acc = m.result().numpy()\n",
    "    logger.info('Evalution: Testing Accuracy: {:.3f}'.format(acc))\n",
    "    history_acc.append(acc)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "    logger.info('Best Accuracy: {:.3f}'.format(best_acc))\n",
    "\n",
    "    if len(history_acc) > params['num_patience'] and is_descending(history_acc):\n",
    "        logger.info('Testing Accuracy not improved over {} epochs, Early Stop'.format(params['num_patience']))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27485659a88f1fd58af2bbdf1eac3305a5af033acb89fa8ad672106ac8ed4a97"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('tf24')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
